{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "pEZkbRD5ripv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjHEvY2L20Fv",
        "outputId": "8da7767f-3524-4b9b-90bd-fd9ca310d065"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of abs_pe : (10, 5)\n",
            "\n",
            "[[ 0.0000e+00  1.0000e+00  0.0000e+00  1.0000e+00  0.0000e+00]\n",
            " [ 8.4147e-01  5.4030e-01  2.5116e-02  9.9968e-01  6.3096e-04]\n",
            " [ 9.0930e-01 -4.1615e-01  5.0217e-02  9.9874e-01  1.2619e-03]\n",
            " [ 1.4112e-01 -9.8999e-01  7.5285e-02  9.9716e-01  1.8929e-03]\n",
            " [-7.5680e-01 -6.5364e-01  1.0031e-01  9.9496e-01  2.5238e-03]\n",
            " [-9.5892e-01  2.8366e-01  1.2526e-01  9.9212e-01  3.1548e-03]\n",
            " [-2.7942e-01  9.6017e-01  1.5014e-01  9.8866e-01  3.7857e-03]\n",
            " [ 6.5699e-01  7.5390e-01  1.7493e-01  9.8458e-01  4.4167e-03]\n",
            " [ 9.8936e-01 -1.4550e-01  1.9960e-01  9.7988e-01  5.0476e-03]\n",
            " [ 4.1212e-01 -9.1113e-01  2.2415e-01  9.7455e-01  5.6786e-03]]\n"
          ]
        }
      ],
      "source": [
        "# ABSOLUTE POSITIONAL EMBEDDING\n",
        "\n",
        "def abs_positional_encoding(length, dimensions):\n",
        "  # 'length' is the length of sequences\n",
        "  # 'dimensions' is the dimension of input embedding\n",
        "\n",
        "  def single_pos_embed(position): #calculating for a certain position\n",
        "      return [position / np.power(10000, 2 * (i // 2) / dimensions)\n",
        "              for i in range(dimensions)]\n",
        "\n",
        "  pos_enc = np.array([single_pos_embed(i) for i in range(length)]) #adding embedding for all positions\n",
        "  pos_enc[:, 0::2] = np.sin(pos_enc[:, 0::2])  # dim 2i application\n",
        "  pos_enc[:, 1::2] = np.cos(pos_enc[:, 1::2])  # dim 2i+1 application\n",
        "  return pos_enc\n",
        "\n",
        "abs_pe = abs_positional_encoding(length = 10,dimensions = 5)\n",
        "print(f\"shape of abs_pe : {abs_pe.shape}\\n\")\n",
        "np.set_printoptions(precision=4)\n",
        "print(abs_pe)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RELATIVE POSITIONAL EMBEDDING\n",
        "\n",
        "def rel_positional_encoding(length,dimensions,calc = 'simple',clip = None):\n",
        "  # 'length' is the length of sequences\n",
        "  # 'dimensions' is the dimension of input embedding\n",
        "  # 'calc' decides the type of relative positional embedding to be used.\n",
        "  # 'calc' = { 'simple' : returns the relative position value itself ..-3,-2,-1,0,1,2,3...\n",
        "  #            'sin'    : return the sinusoidal absolute embedding related to relative position (assumes symmetric relation forward and backward)\n",
        "  #          }\n",
        "  # 'clip' decides after what relative position should it maintain the last relational embedding for the rest.\n",
        "  #        for e.g.: if clip = 3, then for the 6th word, 'calc' = 'simple' gives -3,-3,-3,-2,-1,0,1,2,3,3,3..\n",
        "\n",
        "\n",
        "\n",
        "  # depending upon 'calc' value, deciding the relative embedding calculations\n",
        "\n",
        "  if calc == \"simple\":\n",
        "\n",
        "    def get_w_relativepos(rel): # function that returns a simple relative positional embedding\n",
        "\n",
        "      if clip and abs(rel) > clip: # clipping if specified\n",
        "        return clip * (rel / abs(rel))\n",
        "      else: return rel\n",
        "\n",
        "  elif calc == \"sin\":\n",
        "\n",
        "    def single_pos_embed(rel_position): # function that returns a sinusoidal positional embedding for a single relative position of a word w.r.t another\n",
        "\n",
        "      embed = []\n",
        "      for i in range(dimensions):\n",
        "        if i % 2 == 0: embed.append(np.sin(rel_position / np.power(10000, 2 * (i // 2) / dimensions)))\n",
        "        else: embed.append(np.cos(rel_position / np.power(10000, 2 * (i // 2) / dimensions)))\n",
        "      return embed\n",
        "    # above function can also be replaced with an array containing all possible positional embedding so the repitive calculations can be avoided\n",
        "\n",
        "\n",
        "    def get_w_relativepos(rel):\n",
        "\n",
        "      # index_sin = rel + length\n",
        "      index_sin = rel\n",
        "\n",
        "      if clip and abs(rel) > clip:\n",
        "        # index_sin = (clip * (rel / abs(rel))) + length\n",
        "        index_sin = clip * (rel / abs(rel))\n",
        "\n",
        "      return single_pos_embed(index_sin)\n",
        "\n",
        "  else:\n",
        "    print(\"Invalid Calculation type! exited!\")\n",
        "    return\n",
        "\n",
        "  result = []\n",
        "  for i in range(length): # generationg relative pos embeddings for all positions\n",
        "    result.append([get_w_relativepos(j-i) for j in range(length)]) # generating and adding relative pos embeddings for a word at certain position w.r.t to all words\n",
        "\n",
        "  return np.array(result)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\t******* 'simple' relative positonal embedding ********\")\n",
        "rel_pe = rel_positional_encoding(length = 10, dimensions = 5, calc = \"simple\", clip = 0)\n",
        "print(f\"shape of rel_pe : {rel_pe.shape}\\n\")\n",
        "np.set_printoptions(precision=4)\n",
        "print(f\"The relative embedding a_(0,0) : \\n{rel_pe[0][0]}\")\n",
        "print(f\"The relative embedding a_(1,1) : \\n{rel_pe[1][1]}\\n\")\n",
        "print(f\"The relative embedding a_(1,2) : \\n{rel_pe[1][2]}\")\n",
        "print(f\"The relative embedding a_(1,0) : \\n{rel_pe[1][0]}\")\n",
        "\n",
        "\n",
        "print(f\"\\n\\n\\n\\t******* 'sin' relative positonal embedding ********\")\n",
        "rel_pe = rel_positional_encoding(length = 10, dimensions = 5, calc = \"sin\", clip = 0)\n",
        "print(f\"shape of rel_pe : {rel_pe.shape}\\n\")\n",
        "np.set_printoptions(precision=4)\n",
        "print(f\"The relative embedding a_(0,0) : \\n{rel_pe[0][0]}\")\n",
        "print(f\"The relative embedding a_(1,1) : \\n{rel_pe[1][1]}\\n\")\n",
        "print(f\"The relative embedding a_(1,2) : \\n{rel_pe[1][2]}\")\n",
        "print(f\"The relative embedding a_(1,0) : \\n{rel_pe[1][0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9WsK0WT3gAY",
        "outputId": "42fb5ac3-0432-45f4-fdd0-9d69dd21081e"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t******* 'simple' relative positonal embedding ********\n",
            "shape of rel_pe : (10, 10)\n",
            "\n",
            "The relative embedding a_(0,0) : \n",
            "0\n",
            "The relative embedding a_(1,1) : \n",
            "0\n",
            "\n",
            "The relative embedding a_(1,2) : \n",
            "1\n",
            "The relative embedding a_(1,0) : \n",
            "-1\n",
            "\n",
            "\n",
            "\n",
            "\t******* 'sin' relative positonal embedding ********\n",
            "shape of rel_pe : (10, 10, 5)\n",
            "\n",
            "The relative embedding a_(0,0) : \n",
            "[0. 1. 0. 1. 0.]\n",
            "The relative embedding a_(1,1) : \n",
            "[0. 1. 0. 1. 0.]\n",
            "\n",
            "The relative embedding a_(1,2) : \n",
            "[8.4147e-01 5.4030e-01 2.5116e-02 9.9968e-01 6.3096e-04]\n",
            "The relative embedding a_(1,0) : \n",
            "[-8.4147e-01  5.4030e-01 -2.5116e-02  9.9968e-01 -6.3096e-04]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ROTARY POSITIONAL EMBEDDINGS\n",
        "\n",
        "def rot_positional_encoding(length, dimensions):\n",
        "\n",
        "  thetas = [10000 ** (-2 * (i - 1) / dimensions) for i in range(int(dimensions / 2))] # FInding the fixed theta values\n",
        "\n",
        "  # genrating a single rotational matrix for a certain position\n",
        "  def gen_rotary_pos_embed_matrix(position):\n",
        "    rot_matrix = np.zeros((dimensions, dimensions))\n",
        "    for i in range(int(dimensions / 2)):\n",
        "      rot_matrix[(2*i)][(2*i)] =  np.cos(position * thetas[i])\n",
        "      rot_matrix[(2*i)][(2*i)+1] = - np.sin(position * thetas[i])\n",
        "      rot_matrix[(2*i)+1][(2*i)] =  np.sin(position * thetas[i])\n",
        "      rot_matrix[(2*i)+1][(2*i)+1] =  np.cos(position * thetas[i])\n",
        "    return rot_matrix\n",
        "\n",
        "\n",
        "  output = []\n",
        "  for i in range(length): # calculating the rotational matrices for all positions\n",
        "     output.append(gen_rotary_pos_embed_matrix(i))\n",
        "\n",
        "  return np.array(output)\n",
        "\n",
        "\n",
        "\n",
        "rot_pe = rot_positional_encoding(length = 10, dimensions = 4)\n",
        "print(f\"shape of rot_pe : {rot_pe.shape}\\n\")\n",
        "np.set_printoptions(precision=4)\n",
        "print(f\"rotational matrix for first word :\")\n",
        "print(rot_pe[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVFap0I2rbkU",
        "outputId": "fcc4e8b1-0d6e-49e0-a368-dc113a04db5c"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of rot_pe : (10, 4, 4)\n",
            "\n",
            "rotational matrix for first word :\n",
            "[[ 1. -0.  0.  0.]\n",
            " [ 0.  1.  0.  0.]\n",
            " [ 0.  0.  1. -0.]\n",
            " [ 0.  0.  0.  1.]]\n"
          ]
        }
      ]
    }
  ]
}